{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Random Forest\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   **[Introduction to Random Forest](#1.-Introduction-to-Random-Forest)**\n",
    "2.   **[Foundations of Random Forest](#2.-Foundations-of-Random-Forest)**\n",
    "3.   **[Random Forest Tuning](#3.-Random-Forest-Tuning)**\n",
    "4.   **[Model Validation](#4.-Model-Validation)**\n",
    "5.   **[Exploratory Data Analysis](#5.-Exploratory-Data-Analysis)**\n",
    "6.   **[Model Construction](#6.-Model-Construction)**\n",
    "7.   **[Model Results & Evaluation](#7.-Model-Results-&-Evaluation)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Introduction-to-Random-Forest\"></a>\n",
    "### 1. Introduction to Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest |** Ensemble of decision trees trained on bootstrapped data with randomly selected features\n",
    "\n",
    "**Ensembling |** Ensemble learning refers to aggregating predictive model outputs to make a prediction rather than relying on an individual model\n",
    "\n",
    "**Base Learner |** Each individual model that comprises an ensemble \n",
    "\n",
    "**Weak Learning |** A model that performs slightly better than randomly guessing\n",
    "\n",
    "**Bootstrapping |** Refers to sampling with replacement which is the process of selecting a random item from a dataset and returning it back to the dataset before the next selection, allowing the same item to be selected multiple times.\n",
    "\n",
    "**Bagging |** Bootstrapping + Aggregating \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"2.-Foundations-of-Random-Forest\"></a>\n",
    "### 2. Foundations of Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Concept\n",
    "If you build a bagging ensemble of decision trees but take it one step further by randomizing the features used to train each base learner, the result is called a random forest.\n",
    "\n",
    "Random forest models leverage randomness to reduce the likelihood that a given base learner will make the same mistakes as other base learners. When mistakes between learners are uncorrelated, it reduces both bias and variance. In bagging, this randomization occurs by training each base learner on a sampling of the observations, with replacement.\n",
    "\n",
    "#### 2.2 Sampling affect on predictions\n",
    "The effect of all this sampling is that the base learners each see only a fraction of the possible data that’s available to them and it is possible for model scores to improve with sampling, while also requiring significantly less time to run, since each tree is built from less data.\n",
    "\n",
    "Random forest builds on bagging, taking randomization even further by using only a fraction of the available features to train its base learners. This randomization from sampling often leads to both better performance scores and faster execution times, making random forest a powerful and relatively simple tool in the hands of any data professional."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"3.-Random-Forest-Tuning\"></a>\n",
    "### 3. Random Forest Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest tuning is the same as decision tree tuning which refers to the process of adjusting the hyperparameters to manipulate the structure of a decision trees algorithms with the aim of improving the overall performance on a given task.\n",
    "\n",
    "**Most Important Hyperparameters for Random Forest Tuning:**\n",
    "\n",
    "| Hyperparameter | What it does | Input type | Default Value | Considerations |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `n_estimators` | Specifies the number of trees the model will build in its ensemble | int | 100 | A typical range is 50–500. Consider how much data, how deep the trees are allowed to grow and how many samples are bootstrapped to grow each tree (generally need more trees if they’re shallow, and more trees if your bootstrap sample size is smaller). Also consider if use case has latency requirements. |\n",
    "| `max_depth` | Specifies how many levels tree can have.   If None, trees grow until leaves are pure or until all leaves contain less than min_samples_split samples. | int | None  | Random forest models often use base learners that are fully grown. Restricting tree depth can reduce train/latency times and prevent overfitting. If not None, consider values 3–8. |\n",
    "| `min_samples_split` | Controls threshold below which nodes become leaves  If float, then it represents a percentage (0–1] of max_samples. | int or float | 2 | Consider (a) how many samples are in dataset, and (b) how much of that data to allow each base learner to use (i.e., the value of the max_samples hyperparameter). The fewer samples available, the lesser the number of samples may need to be allowed in each leaf node (otherwise the tree would be very shallow). |\n",
    "| `min_samples_leaf` | A split can only occur if it guarantees a minimum of this number of observations in each resulting node. If float, then it represents a percentage (0–1] of max_samples. | int or float | 1 | Consider (a) how many samples are in your dataset, and (b) how much of that data you're allowing each base learner to use (i.e., the value of the max_samples hyperparameter). The fewer samples available, the lesser the number of samples may need to be allowed  in each leaf node (otherwise the tree would be very shallow). |\n",
    "| `max_features` | Specifies the number of features that each tree randomly selects during training. - If int, then consider max_features features at each split. - If float, then max_features is a fraction and round (max_features * n_features) features are considered at each split. - If “sqrt”, then max_features=sqrt(n_features). - If “log2”, then max_features=log2(n_features). - If None, then max_features=n_features. | {“sqrt”, “log2”, None}, int or float. | “sqrt” | Consider how many features the dataset has and how many trees will be grown. Fewer features sampled during each bootstrap means more base learners would be needed. Small max_features values on datasets with many features mean more unpredictive trees in the ensemble. |\n",
    "| `max_samples` | Specifies the number of samples bootstrapped from the dataset to train each base model If float, then it represents a percentage (0–1] of the dataset.If None, then draw X.shape[0] samples. | int or float | None | Consider the size of your dataset. When working with large datasets, it can be beneficial to limit the number of samples in each tree, because doing so can greatly reduce training time and yet still result in a robust model. For example, 20% of 1 billion may be enough to capture patterns in the data, but if you have 1,000 samples then you’ll probably need to use them all. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"4.-Model-Validation\"></a>\n",
    "### 4. Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation is the set of processes and activities intended to verify that models are performing as expected. Model validation refers to the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations. \n",
    "\n",
    "#### 4.1 Validation Sets \n",
    "The simplest way to maintain the objectivity of the test data is to create another partition in the data—a validation set—and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models.\n",
    "\n",
    "When building a model using a separate validation set, once the final model is selected, best practice is to go back and fit the selected model to all the non-test data (i.e., the training data + validation data) before scoring this final model on the test data.\n",
    "\n",
    "\n",
    "#### 4.2 Cross Validation \n",
    "Cross Validation is a process that uses different portions of the data to test and train a model on different iterations.  \n",
    "\n",
    "Cross-validation makes more efficient use of the training data by splitting the training data into k number of “folds” (partitions), training a model on k – 1 folds, and using the fold that was held out to get a validation score. The training process occurs k times, each time using a different fold as the validation set. At the end, the final validation score is the average of all k scores. This process is also commonly referred to as k-fold cross validation.\n",
    "\n",
    "After a model is selected using cross-validation, that selected model is then refit to the entire training set (i.e., it’s retrained on all k folds combined).\n",
    "\n",
    "Cross-validation reduces the likelihood of significant divergence of the distributions in the validation data from those in the full dataset. For this reason, it’s often the preferred technique when working with smaller datasets, which are more susceptible to randomness. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.\n",
    "\n",
    "#### 4.3 Model Selection\n",
    "Once candidate models have been trained and validated a champion model is selected. Model validation scores factor heavily into this selection but score is seldom the only criterion. Often other factors are also considered. \n",
    "- How explainable is the model? \n",
    "- How complex is it? \n",
    "- How resilient is it against fluctuations in input values? \n",
    "- How well does it perform on data not found in the training data? \n",
    "- How much computational cost does it have to make predictions? \n",
    "- Does it add much latency to any production system? \n",
    "It’s not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Exploratory-Data-Analysis\"></a>\n",
    "### 5. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle as pkl\n",
    " \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Load the dataset into a DataFrame and save in a variable\n",
    "data = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Exploration\n",
    "After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes: \n",
    "\n",
    "*   Exploring data\n",
    "*   Checking for missing values\n",
    "*   Encoding categorical data \n",
    "*   Dropping irrelevant columns\n",
    "*   Renaming columns\n",
    "*   Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, number of columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data type for each column. NB logistic regression models expect numeric data\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1 Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values.\n",
    "# Save DataFrame in variable `data_subset`.\n",
    "data_subset = data.dropna(axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "data_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 10 rows.\n",
    "data_subset.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the island column.\n",
    "data_subset = data_subset.drop(['irrelevant_column'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Encode Data\n",
    "\n",
    "Decision trees require numeric columns so all relevant columns (target and predictor variables) must be converted accordingly through the process of One-hot encoding or Label encoding\n",
    "- Columns requiring special attention should be dealt with separately (special focus on target variable)\n",
    "- Finally all columns can be converted to numeric using `pandas.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent the data in the target variable numerically\n",
    "data_subset['target_variable'] = data_subset['target_variable'].map({\"class_1\": 1, \"class_2\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical columns to numeric.\n",
    "data_subset = pd.get_dummies(data_subset, drop_first = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.4 Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into labels (y) and features (X).\n",
    "y = data_subset[\"target_variable\"]\n",
    "\n",
    "X = data_subset.copy()\n",
    "X = X.drop(\"target_variable\", axis = 1)\n",
    "\n",
    "# Separate into train, validate, test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify= y, random_state = 0)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify= y_train, random_state = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.-Model-Construction\"></a>\n",
    "### 6. Model Construction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine set of hyperparameters\n",
    "cv_params = {'n_estimators' : [50,100], \n",
    "              'max_depth' : [10,50],        \n",
    "              'min_samples_leaf' : [0.5,1], \n",
    "              'min_samples_split' : [0.001, 0.01],\n",
    "              'max_features' : ['sqrt'], \n",
    "              'max_samples' : [.5,.9]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a separate validation set is a common practice in machine learning to evaluate the performance of a model and prevent overfitting.**\n",
    "\n",
    "During the training process, the model is optimized to minimize the error on the training data, and it can start to memorize the training data instead of learning the underlying patterns in the data. This can lead to poor performance on new, unseen data.\n",
    "\n",
    "To evaluate the performance of the model on new data, a separate validation set is used. The validation set is a portion of the original dataset that is not used for training, and it is used to evaluate the performance of the model during the training process.\n",
    "\n",
    "By monitoring the performance of the model on the validation set, we can tune the hyperparameters and adjust the model to prevent overfitting. Once we have a well-tuned model, we can use a final test set, which is another portion of the original dataset that is not used for training or validation, to evaluate the performance of the model on completely unseen data.\n",
    "\n",
    "Therefore, using a separate validation set is important to ensure that the model generalizes well to new data and is not overfitting to the training data. It allows us to optimize the model's performance while ensuring that it is not just memorizing the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```For Model Validation GridsearchCV is going to want to cross-validate the data. if cv parameter were left blank it would split the data into 5  folds for cv by default. Because using a separate validation set it is important to explicitly tell the function how to perform the validation. This includes telling it every row in the training and testing sets. Use a list comprehension to generate a list of the same length as the x_tr data where each value is either -1 or 0. Use this list to indicate to GridSearchCV that each row labeled -1 is training set and each row labeled 0 is in the validation set```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of split indices\n",
    "split_index = [0 if x in X_val.index else -1 for x in X_train.index] \n",
    "\n",
    "# use PredefinedSplit from sklearn.model_selection to provide train/test indices to split data into training and test sets using a predefined scheme\n",
    "custom_split = PredefinedSplit(split_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over specified parameters using GridSearchCV\n",
    "rf_val = GridSearchCV(rf, cv_params, cv=custom_split, refit='f1', n_jobs= -1, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit the model.\n",
    "rf_val.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain optimal parameters.\n",
    "rf_val.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"7.-Model-Results-&-Evaluation\"></a>\n",
    "### 7. Model Results & Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Optimize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected model to predict on test data\n",
    "# Use optimal parameters on GridSearchCV\n",
    "rf_opt = RandomForestClassifier(n_estimators = 50, max_depth = 50, \n",
    "                                min_samples_leaf = 1, min_samples_split = 0.001,\n",
    "                                max_features=\"sqrt\", max_samples = 0.9, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set.\n",
    "y_pred = rf_opt.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Obtain Performance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision score.\n",
    "pc_test = precision_score(y_test, y_pred, pos_label = \"satisfied\")\n",
    "print(\"The precision score is {pc:.3f}\".format(pc = pc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recall score.\n",
    "rc_test = recall_score(y_test, y_pred, pos_label = \"satisfied\")\n",
    "print(\"The recall score is {rc:.3f}\".format(rc = rc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy score.\n",
    "ac_test = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy score is {ac:.3f}\".format(ac = ac_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get F1 score.\n",
    "f1_test = f1_score(y_test, y_pred, pos_label = \"satisfied\")\n",
    "print(\"The F1 score is {f1:.3f}\".format(f1 = f1_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncover which features might be most important by building a feature importance graph\n",
    "importances = model.best_estimator_.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = forest_importances.sort_values(ascending=True)\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot the sorted feature importances with swapped x and y axis\n",
    "sorted_importances.plot.barh(ax=ax)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Relative Feature Importance - Random Forest')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros and cons of performing the model selection using test data instead of a separate validation dataset.\n",
    "\n",
    "Pros: <br />\n",
    "*  The coding workload is reduced.\n",
    "*  The scripts for data splitting are shorter.\n",
    "*  It's only  necessary to evaluate test dataset performance once, instead of two evaluations (validate and test).\n",
    "\n",
    "Cons: <br />\n",
    "* If a model is evaluated using samples that were also used to build or fine-tune that model, it likely will provide a biased evaluation.\n",
    "* A potential overfitting issue could happen when fitting the model's scores on the test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four basic parameters for evaluating the performance of a classification model?\n",
    "\n",
    "1. True positives (TP): These are correctly predicted positive values, which means the value of actual and predicted classes are positive. \n",
    "\n",
    "2. True negatives (TN): These are correctly predicted negative values, which means the value of the actual and predicted classes are negative.\n",
    "\n",
    "3. False positives (FP): This occurs when the value of the actual class is negative and the value of the predicted class is positive.\n",
    "\n",
    "4. False negatives (FN): This occurs when the value of the actual class is positive and the value of the predicted class in negative. \n",
    "\n",
    "**Reminder:** When fitting and tuning classification models, aim to minimize false positives and false negatives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the four scores demonstrate about a model.\n",
    "\n",
    "- Accuracy: The ratio of correctly predicted observations to total observations. \n",
    "- Precision: The ratio of correctly predicted positive observations to total predicted positive observations. \n",
    "- Recall: The ratio of correctly predicted positive observations to all observations in actual class.\n",
    "- F1 score: The harmonic average of precision and recall, which takes into account both false positives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision score on test data set.\n",
    "print(\"\\nThe precision score is: {pc:.3f}\".format(pc = pc_test), \"for the test set,\", \"\\nwhich means of all positive predictions,\", \"{pc_pct:.1f}% prediction are true positive.\".format(pc_pct = pc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall score on test data set.\n",
    "print(\"\\nThe recall score is: {rc:.3f}\".format(rc = rc_test), \"for the test set,\", \"\\nwhich means of which means of all real positive cases in test set,\", \"{rc_pct:.1f}% are  predicted positive.\".format(rc_pct = rc_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score on test data set.\n",
    "print(\"\\nThe accuracy score is: {ac:.3f}\".format(ac = ac_test), \"for the test set,\", \"\\nwhich means of all cases in test set,\", \"{ac_pct:.1f}% are predicted true positive or true negative.\".format(ac_pct = ac_test * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 score on test data set.\n",
    "print(\"\\nThe F1 score is: {f1:.3f}\".format(f1 = f1_test), \"for the test set,\", \"\\nwhich means the test set's harmonic mean is {f1_pct:.1f}%.\".format(f1_pct = f1_test * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** How well does this model perform based on the four scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of results.\n",
    "table = pd.DataFrame()\n",
    "table = table.append({'Model': \"Tuned Decision Tree\",\n",
    "                        'F1':  0.945422,\n",
    "                        'Recall': 0.935863,\n",
    "                        'Precision': 0.955197,\n",
    "                        'Accuracy': 0.940864\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "\n",
    "table = table.append({'Model': \"Tuned Random Forest\",\n",
    "                        'F1':  f1_test,\n",
    "                        'Recall': rc_test,\n",
    "                        'Precision': pc_test,\n",
    "                        'Accuracy': ac_test\n",
    "                      },\n",
    "                        ignore_index=True\n",
    "                    )\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** How does this random forest model compare to any other models that have been constructed to perform the same task."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
