{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Naive Bayes Classification Model\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   **[Introduction to Naive Bayes](#1.-Introduction-to-Naive-Bayes)**\n",
    "2.   **[Foundations of Naive Bayes](#2.-Foundations-of-Naive-Bayes)**\n",
    "3.   **[Model Assumptions](#3.-Model-Assumptions)**\n",
    "4.   **[Model Interpretation](#4.-Model-Interpretation)**\n",
    "5.   **[Exploratory Data Analysis](#5.-Exploratory-Data-Analysis)**\n",
    "6.   **[Model Construction](#6.-Model-Construction)**\n",
    "7.   **[Model Results](#7.-Model-Results)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Introduction-to-Naive-Bayes\"></a>\n",
    "### 1. Introduction to Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes |** A supervised classification technique that is based on Bayes' Theorem with an assumption of independence among predictors\n",
    "\n",
    "**Posterior Probability |** The probability of an event occurring after taking into consideration new information\n",
    "\n",
    "**Response Variable (Dependent Variable) |** The outcome of interest that is being studied and is expected to change based on the values of the independent variables.\n",
    "\n",
    "**Predictor Variables (Independent Variable) |** Variables that are used to predict or explain changes in the value of the response or dependent variable in a statistical model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"2.-Foundations-of-Naive-Bayes\"></a>\n",
    "### 2. Foundations of Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Naive Bayes model is a supervised learning technique used for classification problems. As with all supervised learning techniques, to create a Naive Bayes model you must have a response variable and a set of predictor variables to train the model.\n",
    "\n",
    "The Naive Bayes algorithm is based on Bayes‚Äô Theorem, an equation that can be used to calculate the probability of an outcome or class, given the values of predictor variables. This value is known as the posterior probability.\n",
    "\n",
    "That probability is calculated using three values:\n",
    "\n",
    "- The probability of the outcome overall P(A)\n",
    "- The probability of the value of the predictor variable P(B)\n",
    "- The conditional probability P(B|A) (Note: P(B|A) is interpreted as *the probability of B, given A.*)\n",
    "\n",
    "The probability of the outcome overall, P(A), is multiplied by the conditional probability, P(B|A). This result is then divided by the probability of the predictor variable, P(B), to obtain the posterior probability.\n",
    "\n",
    "**Bayes Theorem |**  $P(A|B)= \\frac{P(B|A)*P(A)}{P(B)}$\n",
    "\n",
    "The goal of Bayes‚Äô Theorem is to find the probability of an event, A, given that another event B is true. In the context of a predictive model, the class label would be A and the predictor variable would be B. P(A) is considered the prior probability of event A before any evidence (feature) is seen. Then, P(A|B) is the posterior probability, or the probability of the class label after the evidence (feature) has been seen.\n",
    "\n",
    "#### Model Implementations\n",
    "There are several implementations of Naive Bayes in scikit-learn, all of which are found in the sklearn.naive_bayes module. Each is optimized for different conditions of the predictor variables.\n",
    "\n",
    "- **BernoulliNB**:      Used for binary/Boolean features\n",
    "- **CategoricalNB**:    Used for categorical features\n",
    "- **ComplementNB**: \tUsed for imbalanced datasets, often for text classification tasks\n",
    "- **GaussianNB**:\t\tUsed for continuous features, normally distributed features\n",
    "- **MultinomialNB**:\tUsed for multinomial (discrete) features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"3.-Model-Assumptions\"></a>\n",
    "### 3. Model Assumptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model assumptions are statements about the data that must be true in order to justify the use of a particular modeling technique\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Naive Bayes Assumptions\n",
    "- **Conditional Independence |** In Naive Bayes, the predictor variables (B and C in the equation above) are assumed to be conditionally independent of each other, given the target variable (A).\n",
    "    - $P(B|C,A) = P(B|A) -> the probability of B, given C and A, is equal to the probability of B, given A.\n",
    "- **Equal Predictive Power |** No predictor variable has any more predictive power than any other predictor. In other words, the individual predictor variables are assumed to contribute equally to the model‚Äôs prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.2 Assumption Violations\n",
    "\n",
    "Even though assumptions are often violated the model still has the potential to perform well.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"4.-Model-Interpretation\"></a>\n",
    "### 4. Model Interpretation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Confusion Matrix\n",
    "\n",
    "A graphical representation of how accurate a classifier is at predicting the labels for a categorical variable \n",
    "\n",
    "####  4.2 Evaluation Metrics \n",
    "- Precision \n",
    "- Recall\n",
    "- Accuracy\n",
    "- F1 score\n",
    "\n",
    "##### 4.2.1 Precision\n",
    "The proportion of positive predictions that were true positives\n",
    "\n",
    "$Precision = \\frac{True Positives}{True Positives + False Positives}$\n",
    "\n",
    "##### 4.2.2 Recall\n",
    "The proportion of positives the model was able to identify correctly \n",
    "\n",
    "$Recall = \\frac{True Positives}{True Positives + False Negatives}$\n",
    "\n",
    "##### 4.2.3 Accuracy\n",
    "The proportion of data points that were correctly categorized\n",
    "\n",
    "$Accuracy = \\frac{True Positives + True Negatives}{Total Predictions}$\n",
    "\n",
    "##### 4.2.4 F-Scores\n",
    "**F1 score:** F1 score is a measurement that combines both precision and recall into a single expression, giving each equal importance. F1 score can range [0, 1], with zero being the worst and one being the best.\n",
    "- $F_1=2* \\frac{precision*recall}{precision+recall}$\n",
    "\n",
    "**F-beta Score:**  In an FùõΩ score, ùõΩ is a factor that represents how many times more important recall is compared to precision. In the case of F1 score, ùõΩ = 1, and recall is therefore 1x as important as precision (i.e., they are equally important). However, an F2 score has ùõΩ = 2, which means recall is twice as important as precision; and if precision is twice as important as recall, then ùõΩ = 0.5.\n",
    "- $F_\\beta=(1+\\beta^2)*\\frac{precision*recall}{(\\beta^2*precision)+recall}$\n",
    "\n",
    "##### 4.2.4 ROC Curves (Receiver Operating Characteristic).\n",
    "Used to visualize the performance of a classifier at different classification thresholds on a graph. A classification threshold in the context of a binary classification is the cutoff threshold for differentiating the positive class from the negative class.\n",
    "- Plots two key concepts\n",
    "    1. True Positive Rate\n",
    "    2. False Positive Rate\n",
    "**The more that the ROC curve hugs the top left corner of the plot, the better the model does at classifying the data.**\n",
    "\n",
    "##### 4.2.5 AUC \n",
    "Stands for Area Under ROC Curve. Provides an aggregate measure of performance across all possible classification thresholds. AUC ranges in value from 0.0 t0 1.0. A model that is 100% wrong has an AUC of 0 and a model predicting 100% correct has AUC of 1.0\n",
    "- An AUC of less than 0.5 indicates that the model performs worse than a random classifier \n",
    "- Python function: `metrics_roc_auc_score(y_test,y_pred)`\n",
    "\n",
    "#### 4.3 Considerations when choosing metrics\n",
    "\n",
    "##### 4.3.1 When to use Precision\n",
    "Using precision as an evaluation metric is especially helpful in contexts where the cost of a false positive is quite high and much higher than the cost of a false negative. For example, in the context of email spam detection, a false positive (predicting a non-spam email as spam) would be more costly than a false negative (predicting a spam email as non-spam). A non-spam email that is misclassified could contain important information, such as project status updates from a vendor to a client or assignment deadline announcements from an instructor to a class of students. \n",
    "\n",
    "##### 4.3.1 When to use Recall\n",
    "Using recall as an evaluation metric is especially helpful in contexts where the cost of a false negative is quite high and much higher than the cost of a false positive. For example, in the context of fraud detection among credit card transactions, a false negative (predicting a fraudulent credit card charge as non-fraudulent) would be more costly than a false positive (predicting a non-fraudulent credit card charge as fraudulent). A fraudulent credit card charge that is misclassified could lead to the customer losing money, undetected.\n",
    "\n",
    "##### 4.3.1 When to use Accuracy\n",
    "It is helpful to use accuracy as an evaluation metric when you specifically want to know how much of the data at hand has been correctly categorized by the classifier. Another scenario to consider: accuracy is an appropriate metric to use when the data is balanced, in other words, when the data has a roughly equal number of positive examples and negative examples. Otherwise, accuracy can be biased. For example, imagine that 95% of a dataset contains positive examples, and the remaining 5% contains negative examples. Then you train a logistic regression classifier on this data and use this classifier predict on this data. If you get an accuracy of 95%, that does not necessarily indicate that this classifier is effective. Since there is a much larger proportion of positive examples than negative examples, the classifier may be biased towards the majority class (positive) and thus the accuracy metric in this context may not be meaningful. When the data you are working with is imbalanced, consider either transforming it to be balanced or using a different evaluation metric other than accuracy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Exploratory-Data-Analysis\"></a>\n",
    "### 5. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and modules.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load the dataset into a DataFrame and save in a variable\n",
    "data = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, number of columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data type for each column. NB logistic regression models expect numeric data\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Model Preparation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 Isolate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the y (target) variable.\n",
    "y = data['Dependant_Variable']\n",
    "\n",
    "# Define the X (predictor) variables.\n",
    "X = data.drop('Dependant_Variable', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of target data.\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of predictor variables.\n",
    "X.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the split operation on data.\n",
    "# Assign the outputs as follows: X_train, X_test, y_train, y_test.\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape (rows, columns) of the output from the train-test split.\n",
    "\n",
    "# Print the shape of X_train.\n",
    "print(X_train.shape)\n",
    "\n",
    "# Print the shape of X_test.\n",
    "print(X_test.shape)\n",
    "\n",
    "# Print the shape of y_train.\n",
    "print(y_train.shape)\n",
    "\n",
    "# Print the shape of y_test.\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.-Model-Construction\"></a>\n",
    "### 6. Model Construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Consider which Naive Bayes algorithm should be used:\n",
    "- **BernoulliNB**:      Used for binary/Boolean features\n",
    "- **CategoricalNB**:    Used for categorical features\n",
    "- **ComplementNB**: \tUsed for imbalanced datasets, often for text classification tasks\n",
    "- **GaussianNB**:\t\tUsed for continuous features, normally distributed features\n",
    "- **MultinomialNB**:\tUsed for multinomial (discrete) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign `nb` to be the appropriate implementation of Naive Bayes.\n",
    "nb = naive_bayes.GaussianNB()\n",
    "\n",
    "# Fit the model on your training data.\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Apply your model to predict on your test data. Call this \"y_pred\".\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"7.-Model-Results\"></a>\n",
    "### 7. Model Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1 Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results by printing accuracy, precision, recall and F1 score\n",
    "# Print your accuracy score.\n",
    "print('Accuracy score:'), print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Print your precision score.\n",
    "print('Precision score:'), print(metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Print your recall score.\n",
    "print('Recall score:'), print(metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "# Print your f1 score.\n",
    "print('F1 score:'), print(metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "# Option 2: better formatted output\n",
    "\n",
    "# print(\"Accuracy:\", \"%.6f\" % metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"Precision:\", \"%.6f\" % metrics.precision_score(y_test, y_pred))\n",
    "# print(\"Recall:\", \"%.6f\" % metrics.recall_score(y_test, y_pred))\n",
    "# print(\"F1 Score:\", \"%.6f\" % metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the accuracy score for your model, and what does this tell you about the success of the model's performance?\n",
    "\n",
    "**Question:** What are the precision and recall scores for your model, and what do they mean? Is one of these scores more accurate than the other?\n",
    "\n",
    "**Question:** What is the F1 score of your model, and what does this score suggest about this model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a confusion matrix for more clarity\n",
    "cm = metrics.confusion_matrix(y_test, y_pred, labels= nb.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix= cm, display_labels= nb.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do you notice when observing your confusion matrix, and does this correlate to any of your other calculations?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
